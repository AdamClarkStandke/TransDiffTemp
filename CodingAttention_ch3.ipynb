{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNl8QJatIJmqb8ZeRdFcxrb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention\n",
        "\n",
        "This notebook will implement chapter three of the book [Build a Large Language Model (From Scratch) by Sebastian Raschka](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167) which deals with one of the most important architectural aspects of Large Language Models (LLMs)-[**Attention**](https://arxiv.org/abs/1706.03762). Attention is the main architectural feature of [transformers](https://en.wikipedia.org/wiki/Transformer_(deep_learning) that give LLMs superior performance compared to other [AutoRegressive methods](https://www.geeksforgeeks.org/nlp/autoregressive-models-in-natural-language-processing/) especially when it comes to the task of text generation. Which will be the primary focus of this notebook.\n",
        "\n",
        "The chapter covers **self-attention**, **causal-attention** and **multi-head attention** the three main attention mechanisms used in today's transformer architectures. In the following sections, we will ultimatley be calculating the **context vectors** (i.e., an enriched embedding vector that incorporates information from all of the other elements in the sentence/sequence) for the following sentence: **Your journey starts with one step**"
      ],
      "metadata": {
        "id": "4QQt1yR32_Gd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jwr35aCkfUDu"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input sentence: 2d martrix of (6, 3) shape\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your     (x1)\n",
        "   [0.55, 0.87, 0.66],   # journey  (x2)\n",
        "   [0.57, 0.85, 0.64],   # starts   (x3)\n",
        "   [0.22, 0.58, 0.33],   # with     (x4)\n",
        "   [0.77, 0.25, 0.10],   # one      (x5)\n",
        "   [0.05, 0.80, 0.55]]   # step     (x6)\n",
        ")"
      ],
      "metadata": {
        "id": "PwbANzT2BNuM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Attention\n",
        "\n",
        "### Self-Attention No Weights\n",
        "\n",
        "Self-attention refers to computing attention for different positions within a single input sequence. Generally this refers to the **attention weights** which weigh the importance of different parts/elements of the input sequence i.e. which words play a more *important role* than other words in the sequence. As part of this attention process the goal is to calculate **context vectors** for each part/element of the sequence. These context vectors create an enriched embedding vector that incorporates information from all other parts/elements of the sequence. This is how LLMs are able to understand the relationships and relevance of different words to each other in the sentence without being explicitly programmed on how to do so.  \n",
        "\n",
        "The method of calculating self-attention starts as follows:\n",
        "\n",
        "1.  Choose a current word/token as the **query vector** i.e., $x_1=q_1$\n",
        "2.  Calculate **attention scores **between the query vector and all other input words/tokens i.e., $ω_j=\\forall j, token_j\\cdot q_1$\n",
        "3.  Normalize the current attention score to obtain the current **attention weight** i.e., $α_1=softmax(\\omega_j)=\\frac{e^{\\omega_1}}{\\sum_{j=1}^{n}e^{\\omega_j}}$\n",
        "4.  Repeat step 3 for all other words/tokens i.e. calculate all attention weights $\\alpha_{ij}$\n",
        "5. Once all attention weights have been calculated i.e., $α_{ij}$ the **context vector** is calcualted which incorporates all of the information from all of the other words/tokens in the sequence. This is done by multipling each input word/token by its corresponding attention weight and summing the results i.e. $z_{1} = \\sum_{j=1}^{n}\\alpha_{1j} \\cdot token_j$\n",
        "6. Repeat step 1 with a different word/token as the query vector and exit when all context vectors for the input sequence have been computed.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Self-Attention with Weights\n",
        "\n",
        "To implement self-attention with neural networks, you need trainable weights that can be updated and changed based on the training task.\n",
        "\n",
        "*Importantly the trainable weights of the neural network are not to be confused with the attention weights of the context vector since the trainable weights define the connection of the neural network, while the attention weights of the context vector define the connections/relationships of the sequence*.\n",
        "\n",
        "The trainable weights of the network are represented by the following three matrices:\n",
        "*   $W_q$\n",
        "*   $W_k$\n",
        "*   $W_v$\n",
        "\n",
        "Generally these three weight matrices are generated by three different fully connected neural networks and are used to project/transform the words/tokens into a lower dimensional vector space defined by the following vectors:\n",
        "*   Query\n",
        "*   Key\n",
        "*   Value\n",
        "\n",
        "After doing so, these vectors are used to calculate self-attention using the same steps as above except with a few additional steps that take into account the [linear transformation](https://en.wikipedia.org/wiki/Transformation_matrix) from the orginial embedding space to the new embedding space. This is done by taking the square root of the key vector's dimension and using it as a scaler during normalization.\n",
        "\n",
        "Accordingly, the method of calculating self-attention with trainable weights starts as follows:\n",
        "1. Select current word/token i.e., $x_1$ and calculate the **Query** vector i.e., $q_1=W_qx_1$\n",
        "2. Calculate the **Key** and **Value** matrices for all of the other words/tokens in the sequence i.e., $K=W_kX, \\;$ $V=W_vX$\n",
        "3. Calculate the **attention scores** for current word/token i.e. $\\omega_1=q_1K$\n",
        "4. Normalize the attention scores for current word/token by taking the square root of the key vector's dimension within the softmax function to obtain the **attention weights** i.e., $α_1=softmax(\\omega_1/\\sqrt{d_{k}})$\n",
        "5. Calculate the **context vector** for the current word/token which incorporates all of the information from all of the other words/tokens in the sentence. This is done by multipling the **Value** vector by the **attention weights** i.e. $z_{1} = \\alpha_{1}V$\n",
        "6. Repeat step 1 with a different word/token as the query vector and exit when all context vectors for the input sequence have been computed.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hD6H5MKV3PX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating one context vector\n",
        "\n",
        "\n",
        "d_in = inputs.shape[1] #input/starting embedding dim is 3\n",
        "d_out = 2 # set output embedding dim size as 2\n",
        "\n",
        "# intializing the three weight matrices W_Q, W_K, W_V\n",
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "# 1. compute query vector for current token\n",
        "x_2 = inputs[1, :] # select current token i.e. 'journey'\n",
        "query_2 = x_2 @ W_query\n",
        "\n",
        "# 2. compute all key and value matrices for all tokens\n",
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "\n",
        "# 3. compute attention scores for current token\n",
        "attn_scores_2 = query_2 @ keys.T\n",
        "\n",
        "# 4. transform attention scores into scaled attention weights for current token\n",
        "d_k = keys.shape[-1] # embedding dimension for key vector\n",
        "attn_weights_2 = torch.softmax(attn_scores_2/d_k**0.5, dim=-1)\n",
        "\n",
        "# 5. calculate context vector for current token\n",
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(f\"Context Vector for journey: {context_vec_2}\")"
      ],
      "metadata": {
        "id": "Rr_lHW5ikpSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8036ad9b-5cf1-404e-d035-22af18f07c21"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector for journey: tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Attention for Entire Input Sequence"
      ],
      "metadata": {
        "id": "XovC92c5MCRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class for calculating self-attention for entire input sequence\n",
        "class SelfAttention(torch.nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, X):\n",
        "    queries = self.W_query(X)\n",
        "    keys = self.W_key(X)\n",
        "    values = self.W_value(X)\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weight = torch.nn.functional.softmax(\n",
        "        input=attn_scores/keys.shape[-1]**0.5,\n",
        "        dim=-1\n",
        "    )\n",
        "    context_vec = attn_weight @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "CWdailmOMibb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "self_attention = SelfAttention(d_in=3, d_out=2)\n",
        "print(f\"Context Matrix for all words:\\n {self_attention(inputs).detach().numpy()}\")"
      ],
      "metadata": {
        "id": "PawmZOiQUpx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda3280b-9b57-43f6-e277-05d7f796b374"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Matrix for all words:\n",
            " [[-0.07389025  0.07128991]\n",
            " [-0.07481073  0.0703093 ]\n",
            " [-0.07485619  0.07024166]\n",
            " [-0.07600163  0.06845011]\n",
            " [-0.07632761  0.06794281]\n",
            " [-0.07544428  0.06930492]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Causal-Attention\n",
        "\n",
        "**Causal-Attention** (aka Masked-Attention) is about hiding future words in the input sequence.\n",
        "\n",
        "*Importantly: While self-attention considers all of the tokens/words when predicting the next token/word in the sequence. Causal-attention only considers previous tokens/words when predicting the next token/word in the sequence.*\n",
        "\n",
        "Generally, both forms of attention are used to train transformer models. To achieve **Causal-Attention** two things are generally used:\n",
        "1.   [Masks](https://en.wikipedia.org/wiki/Triangular_matrix)\n",
        "2.   [Dropout](https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
        "\n",
        "**Masks** (in most transformers) are generally lower-triangular matricies meaning that the attention weights above the main diagonal are hidden. The book [Build a Large Language Model (From Scratch) by Sebastian Raschka](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167) details two ways of constructing attention masks. The first way is to:\n",
        "1.  create a mask with 0's above the diagonal\n",
        "2.  multiply the mask with the attention weights\n",
        "3.  renormalize the attention weights\n",
        "\n",
        "This method is generally called Simple Masked Attention (SMA) because it does not take advantage of the mathematical properties of the softmax function when dealing with -$∞$ values.\n",
        "\n",
        "The more efficient and generally accepted method is creating a mask with -$∞$ values above the diagonal. This is done because the softmax function as defined mathematically is: $\\frac{e^{z_{i}}}{\\sum_{j=1}^{K}e^{z_{j}}}$ and when raised to -$∞$ amounts to $lim_{z\\to -\\infty} e^{z}=\\frac{1}{e^{\\infty }}=\\frac{1}{\\infty }=0$. So instead of multiplying the mask by the attention weights and then having to renormalize the attention weights, we can just mask the attention scores with -$∞$ values and then apply the softmax function to create the normalized attention weights in one step.This is generally mathematically represented as: $softmax(\\frac{QK^{T}}{\\sqrt{d}}+M)$\n",
        "\n",
        "**Dropout** is generally the last piece to creating Causal-Attention. As mentioned before the whole goal with Causal-Attention is to learn [causal relationships](https://en.wikipedia.org/wiki/Causality) i.e., x causes y. Masking does this by hiding future information with respect to the current input; or in other words, making sure that the current prediction is based only on the previous information. Dropout is used to make sure that these *causal relationships are learned* and not only [memorized](https://arxiv.org/html/2406.03880v1) by the model. Dropout does this by randomly removing attention scores and/or weights. In this case dropout is only applied to the attention weights.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wmb0MYTd3dMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating attention weight matrix for all words\n",
        "query = self_attention.W_query(inputs)\n",
        "keys = self_attention.W_key(inputs)\n",
        "att_score = query @ keys.T\n",
        "att_weight = torch.nn.functional.softmax(input=(att_score/keys.shape[-1]**0.5), dim=-1)\n",
        "print(f\"Attention Weight Matrix for all words:\\n {att_weight.detach().numpy()}\")"
      ],
      "metadata": {
        "id": "RrBc4NSpWJ6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd56c11d-3950-4c73-d69e-afed913b491e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weight Matrix for all words:\n",
            " [[0.19212602 0.1646463  0.16516064 0.15499417 0.17211477 0.15095802]\n",
            " [0.20412546 0.16588287 0.16621484 0.14957766 0.16645327 0.1477459 ]\n",
            " [0.20356156 0.16592424 0.16624875 0.14981547 0.16641727 0.14803267]\n",
            " [0.18688802 0.1666883  0.16683646 0.15710352 0.16609134 0.15639237]\n",
            " [0.18304484 0.16685854 0.16695702 0.1588405  0.16582507 0.15847409]\n",
            " [0.19347237 0.16633299 0.16656809 0.15418623 0.16656083 0.15287954]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Masked Attention"
      ],
      "metadata": {
        "id": "gZQVbEZtL71N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. create a mask with 0's above the diagonal\n",
        "context_length = 6 # input sequence context length\n",
        "mask_ones = torch.tril(torch.ones((context_length, context_length))) # create mask\n",
        "print(f\"Simple Mask: \\n {mask_ones.detach().numpy()}\")"
      ],
      "metadata": {
        "id": "zqJQ_ZhNBsvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beda2e61-2646-4266-df71-72702267c7d8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple Mask: \n",
            " [[1. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. multiply the mask with the attention weights\n",
        "mask_attn = att_weight*mask_ones\n",
        "print(f\"Masked Attention-Weights: \\n {mask_attn.detach().numpy()}\")"
      ],
      "metadata": {
        "id": "S1I4DBQeaj39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03988ad9-5ded-4408-af10-f909c2224317"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masked Attention-Weights: \n",
            " [[0.19212602 0.         0.         0.         0.         0.        ]\n",
            " [0.20412546 0.16588287 0.         0.         0.         0.        ]\n",
            " [0.20356156 0.16592424 0.16624875 0.         0.         0.        ]\n",
            " [0.18688802 0.1666883  0.16683646 0.15710352 0.         0.        ]\n",
            " [0.18304484 0.16685854 0.16695702 0.1588405  0.16582507 0.        ]\n",
            " [0.19347237 0.16633299 0.16656809 0.15418623 0.16656083 0.15287954]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. renormalize the attention weights\n",
        "row_sums=mask_attn.sum(dim=-1, keepdim=True)\n",
        "mask_normalized_attn = mask_attn/row_sums\n",
        "print(f\"Normalized Masked Attention-Weights: \\n {mask_normalized_attn.detach().numpy()}\")"
      ],
      "metadata": {
        "id": "G9eNoKFljAhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a6c11b-7fd5-4a2e-85ee-cc21f8fc54c8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Masked Attention-Weights: \n",
            " [[1.         0.         0.         0.         0.         0.        ]\n",
            " [0.551678   0.44832197 0.         0.         0.         0.        ]\n",
            " [0.3799672  0.30971354 0.31031927 0.         0.         0.        ]\n",
            " [0.27584285 0.24602847 0.24624716 0.23188154 0.         0.        ]\n",
            " [0.21751536 0.1982809  0.19839793 0.18875293 0.1970528  0.        ]\n",
            " [0.19347237 0.16633299 0.16656809 0.15418623 0.16656083 0.15287954]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# context matrix from SMA\n",
        "values = self_attention.W_value(inputs)\n",
        "context_vec = mask_normalized_attn@ values\n",
        "print(f\"Context Matrix from SMA: \\n {context_vec.detach().numpy()}\")"
      ],
      "metadata": {
        "id": "0YU6FhSul3GE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c23dbf-4989-4602-a78b-c9ff0bc849ab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Matrix from SMA: \n",
            " [[-0.08721808  0.02858998]\n",
            " [-0.09906914  0.05009485]\n",
            " [-0.09994501  0.06334987]\n",
            " [-0.0982549   0.04894815]\n",
            " [-0.05144592  0.10984372]\n",
            " [-0.07544428  0.06930492]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Negative Infinity Masked Attention"
      ],
      "metadata": {
        "id": "uGbXSpKuN0p4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. create attention score mask with -infs above the diagonal\n",
        "mask = torch.triu(torch.ones((context_length, context_length)), diagonal=1)\n",
        "mask = att_score.masked_fill(mask.bool(), value=-torch.inf) # attention score mask\n",
        "print(f\"Masked Attention Scores: \\n {mask.detach().numpy()}\")"
      ],
      "metadata": {
        "id": "lKsVX0x1WCgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c390a46-7877-4b1a-d441-1a093dc1e3c3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masked Attention Scores: \n",
            " [[0.2899089        -inf       -inf       -inf       -inf       -inf]\n",
            " [0.4656424  0.17225963       -inf       -inf       -inf       -inf]\n",
            " [0.45943564 0.17031771 0.17308104       -inf       -inf       -inf]\n",
            " [0.26415503 0.10239156 0.10364803 0.01864095       -inf       -inf]\n",
            " [0.21828783 0.08735328 0.08818767 0.01770909 0.0785667        -inf]\n",
            " [0.34078205 0.12703359 0.12903105 0.01979299 0.12896936 0.00775672]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Normalized Masked Attention-Weights\n",
        "att_weight_2 = torch.nn.functional.softmax(input=(mask/keys.shape[-1]**0.5), dim=-1)\n",
        "print(f\"Normalized Masked Attention-Weights: \\n {att_weight_2.detach().numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxDgzlZhetmW",
        "outputId": "4a386bdd-e0ed-4cc4-d704-564e339c0a27"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Masked Attention-Weights: \n",
            " [[1.         0.         0.         0.         0.         0.        ]\n",
            " [0.551678   0.44832197 0.         0.         0.         0.        ]\n",
            " [0.37996718 0.3097135  0.31031924 0.         0.         0.        ]\n",
            " [0.27584285 0.24602845 0.24624714 0.23188154 0.         0.        ]\n",
            " [0.2175154  0.19828095 0.19839796 0.18875296 0.19705284 0.        ]\n",
            " [0.19347237 0.16633299 0.16656809 0.15418623 0.16656083 0.15287954]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# context matrix from -inf mask\n",
        "values = self_attention.W_value(inputs)\n",
        "context_vec = att_weight_2@ values\n",
        "print(f\"Context Matrix from -inf mask: \\n {context_vec}\")"
      ],
      "metadata": {
        "id": "RhLzg3wplV9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ea39b8-98a6-4213-c049-557d5fd023f3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Matrix from -inf mask: \n",
            " tensor([[-0.0872,  0.0286],\n",
            "        [-0.0991,  0.0501],\n",
            "        [-0.0999,  0.0633],\n",
            "        [-0.0983,  0.0489],\n",
            "        [-0.0514,  0.1098],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout"
      ],
      "metadata": {
        "id": "UlNnkjovReQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = torch.nn.Dropout(0.5)\n",
        "print(f\"Dropout applied to -inf Attention-Weights: \\n {dropout(att_weight_2).detach().numpy()}\")"
      ],
      "metadata": {
        "id": "USiUXT9VqyEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "745d1deb-76c1-4ddf-8fb9-97242eba9a8b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout applied to -inf Attention-Weights: \n",
            " [[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.89664394 0.         0.         0.         0.        ]\n",
            " [0.75993437 0.619427   0.6206385  0.         0.         0.        ]\n",
            " [0.5516857  0.         0.         0.         0.         0.        ]\n",
            " [0.4350308  0.3965619  0.39679593 0.         0.39410567 0.        ]\n",
            " [0.         0.         0.33313617 0.         0.         0.30575907]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Causal-Attention for Entire Input Sequence"
      ],
      "metadata": {
        "id": "T5oU2-BuI_1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(torch.nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones((context_length, context_length)), diagonal=1)\n",
        "        )\n",
        "  def forward(self, X):\n",
        "    batch, num_tokes, d_in = X.shape # batched input\n",
        "    queries = self.W_query(X)\n",
        "    keys = self.W_key(X)\n",
        "    values = self.W_value(X)\n",
        "    # attention score\n",
        "    attn_scores = queries @ keys.transpose(1,2)\n",
        "    # -inf masked attention weights\n",
        "    mask_attn_scores = attn_scores.masked_fill_(self.mask.bool()[:num_tokes, :num_tokes], value=-torch.inf)\n",
        "    attn_weights = torch.nn.functional.softmax(\n",
        "        input=mask_attn_scores/keys.shape[-1]**0.5,\n",
        "        dim=-1\n",
        "    )\n",
        "    # dropout\n",
        "    attn_weights=self.dropout(attn_weights)\n",
        "    # context matrix\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "cgKftVKlxQeI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch input to have shape: (2, 6, 3)\n",
        "batch = torch.stack((inputs, inputs), dim=0)"
      ],
      "metadata": {
        "id": "PS_-1G489Fqt"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in=3, d_out=2, context_length=context_length, dropout=0.5)\n",
        "context_vecs = ca(batch)\n",
        "print(f\"Batched Context Matrices: \\n\\n {context_vecs.detach().numpy()}\")"
      ],
      "metadata": {
        "id": "L0fk878G9XcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c83d3cd-e030-4fe4-fff8-712efc17f141"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batched Context Matrices: \n",
            "\n",
            " [[[-0.90384054  0.44320962]\n",
            "  [-0.4367989   0.21418986]\n",
            "  [-0.48492774 -0.13410191]\n",
            "  [-0.58335876  0.00813284]\n",
            "  [-0.62186474 -0.05263354]\n",
            "  [-0.14171308 -0.05048606]]\n",
            "\n",
            " [[ 0.          0.        ]\n",
            "  [-1.1748701   0.0115522 ]\n",
            "  [-0.7732556   0.00728327]\n",
            "  [-0.9139531  -0.27685684]\n",
            "  [-0.76786053 -0.07353682]\n",
            "  [-0.6748546  -0.09838524]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention\n",
        "\n",
        "Multi-head attention is just dividing the above attention methods into multiple heads for parallelization purposes. Each head calculates its own context matrix and the results are  then combined into a single context matrix. The book [Build a Large Language Model (From Scratch) by Sebastian Raschka](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167) implements weight splitting multi-head attention in which the number of heads and their dimensions are factored into the Query, Key and Value matrices."
      ],
      "metadata": {
        "id": "TOJpUTDwCTDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAtt(torch.nn.Module):\n",
        "  def __init__(self, d_in, d_out, dropout, conlen, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out//num_heads\n",
        "    self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "    self.out = torch.nn.Linear(d_out, d_out, bias=qkv_bias)\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones((conlen, conlen)), diagonal=1)\n",
        "        )\n",
        "\n",
        "\n",
        "  def forward(self, X):\n",
        "    batch, num_tokes, d_in = X.shape\n",
        "    queries = self.W_query(X)\n",
        "    keys = self.W_key(X)\n",
        "    values = self.W_value(X)\n",
        "\n",
        "    keys=keys.view(batch, num_tokes, self.num_heads, self.head_dim)\n",
        "    values=values.view(batch, num_tokes, self.num_heads, self.head_dim)\n",
        "    queries=queries.view(batch, num_tokes, self.num_heads, self.head_dim)\n",
        "\n",
        "    keys=keys.transpose(1, 2)\n",
        "    values=values.transpose(1, 2)\n",
        "    queries=queries.transpose(1, 2)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(2,3)\n",
        "    mask_attn_scores = attn_scores.masked_fill_(self.mask.bool()[:num_tokes, :num_tokes], value=-torch.inf)\n",
        "    attn_weights = torch.nn.functional.softmax(\n",
        "        input=mask_attn_scores/keys.shape[-1]**0.5,\n",
        "        dim=-1\n",
        "    )\n",
        "    attn_weights=self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = (attn_weights @ values).transpose(1,2)\n",
        "    context_vec = context_vec.contiguous().view(\n",
        "        batch, num_tokes, self.d_out\n",
        "    )\n",
        "    context_vec = self.out(context_vec)\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "puLzSGSPCvAd"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 6\n",
        "multi = MultiHeadAtt(\n",
        "    d_in=3,\n",
        "    d_out=2,\n",
        "    dropout=0.2,\n",
        "    conlen=context_length,\n",
        "    num_heads=2\n",
        ")\n",
        "context_vec = multi(batch)\n",
        "print(f\"First head Context Matrix: \\n{context_vec[0, :, :].detach().numpy()}\")\n",
        "print()\n",
        "print(f\"Second head Context Matrix: \\n{context_vec[1, :, :].detach().numpy()}\")"
      ],
      "metadata": {
        "id": "guxsA7BhednI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a521ae2-853b-4ca7-da9c-dc962c9be162"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First head Context Matrix: \n",
            "[[0.28226054 0.04943101]\n",
            " [0.27218515 0.11528094]\n",
            " [0.27345967 0.13080472]\n",
            " [0.20599014 0.02699874]\n",
            " [0.0383919  0.12273508]\n",
            " [0.2399512  0.07629689]]\n",
            "\n",
            "Second head Context Matrix: \n",
            "[[0.28226054 0.04943101]\n",
            " [0.2923506  0.0804213 ]\n",
            " [0.02250692 0.1599009 ]\n",
            " [0.18400052 0.07392472]\n",
            " [0.0769636  0.1050854 ]\n",
            " [0.19460025 0.08155501]]\n"
          ]
        }
      ]
    }
  ]
}