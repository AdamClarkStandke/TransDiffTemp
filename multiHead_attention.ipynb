{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN21szYbg56Njhba7ijgxgU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamClarkStandke/TransDiffTemp/blob/main/multiHead_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention found in Orginal Transformer Paper"
      ],
      "metadata": {
        "id": "6kWGdESJK0dn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "x0ShTNSf1RCp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Represent the Input\n",
        "\n",
        "As detailed in the book [Transformers for NLP by Denis Rothman](https://www.packtpub.com/en-us/product/transformers-for-natural-language-processing-second-edition-9781803247335?type=print&gad_source=1&gclid=Cj0KCQiA5-uuBhDzARIsAAa21T_Vro5RuNsURN8t_PUHd6aybZm0mi2BApbIxHRjw-qbjQxcrWbvRL8aApJMEALw_wcB):\n",
        "\n",
        "> The input of the attention mechanism we are building is like the orginal paper [Attention is all you need](https://arxiv.org/pdf/1706.03762v1.pdf) in which $d_{model}=512$ [i.e. the positional embedding space]\n",
        "\n",
        "In this example the prompt is the following sentence: **\"I love you\"**\n",
        "\n",
        "In this case *x* contains 3 inputs (i.e., the three words) embedded in the orginal 512 positional embedding space. Giving a 3 by 512 dimensional matrix.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2LGt4Qsp6ngi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Step 1: Input=3 tokenized words, d_model=512\")\n",
        "x = np.random.randint(1, 10, size=(3, 512))\n",
        "print(f\"I: {x[0]}\")\n",
        "print(f\"love: {x[1]}\")\n",
        "print(f\"you: {x[2]}\")\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpKyw9UdKAdH",
        "outputId": "00dec8e6-d24e-4b47-fad4-7cac85ccc2b5"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Input=3 tokenized words, d_model=512\n",
            "I: [8 5 6 7 7 7 2 2 5 5 7 5 5 1 1 9 3 2 9 7 1 1 8 7 1 7 7 1 1 6 1 3 2 6 5 9 3\n",
            " 5 2 6 2 9 4 1 3 3 5 2 1 8 7 6 5 9 5 7 3 2 9 8 5 3 5 1 8 5 2 5 8 3 2 3 6 7\n",
            " 7 1 6 7 2 1 9 1 7 4 5 6 9 6 6 6 8 2 2 8 5 3 8 1 9 5 3 6 4 6 7 6 4 7 5 4 7\n",
            " 7 4 6 7 8 7 3 4 1 4 9 7 6 8 9 4 1 3 4 2 9 6 6 8 8 2 1 5 3 2 1 9 3 4 2 6 9\n",
            " 8 7 7 4 6 8 8 5 1 2 5 8 9 3 6 2 8 5 9 4 6 8 5 8 6 2 3 1 4 2 1 2 2 7 1 4 1\n",
            " 7 4 6 6 8 1 4 3 3 6 6 8 8 2 2 4 2 7 7 1 3 5 3 5 5 1 3 8 7 8 9 5 8 9 6 3 6\n",
            " 5 1 9 3 3 2 2 8 4 2 5 2 9 8 1 1 5 5 2 5 9 8 4 2 3 9 5 4 3 4 2 5 4 5 9 3 1\n",
            " 2 3 1 9 9 7 6 4 1 8 2 5 6 3 7 3 9 4 9 2 8 3 2 9 3 5 4 6 5 9 1 6 8 3 7 7 6\n",
            " 5 1 5 7 2 2 7 1 2 8 7 3 4 5 6 4 4 5 3 1 8 1 1 7 8 6 2 8 8 7 9 4 5 4 5 5 1\n",
            " 1 6 1 7 5 5 3 4 5 3 2 4 3 3 1 8 3 8 6 1 3 6 6 2 2 2 8 3 5 7 6 6 9 8 8 1 6\n",
            " 8 2 1 1 6 7 7 6 7 7 4 2 8 8 1 4 1 3 8 4 6 3 4 9 3 9 9 7 8 4 7 2 3 8 4 1 6\n",
            " 7 5 3 9 5 9 6 3 4 8 5 8 1 6 1 5 3 2 2 6 4 7 1 9 1 8 4 8 9 7 3 6 5 9 9 3 7\n",
            " 8 4 1 4 4 5 9 7 7 3 7 3 6 6 5 9 6 5 2 6 8 5 1 8 4 2 3 3 3 5 4 2 2 9 2 2 3\n",
            " 6 4 4 3 1 2 7 5 2 9 6 3 6 8 9 1 9 9 2 7 5 6 2 4 3 8 4 3 5 2 5]\n",
            "love: [5 9 9 2 6 1 5 9 9 1 4 9 1 3 2 1 6 7 8 6 5 6 5 5 1 2 7 4 2 1 8 1 7 4 4 8 8\n",
            " 8 1 2 8 2 5 2 1 6 4 9 5 8 7 1 5 1 9 8 4 2 2 2 6 6 9 2 7 5 9 2 1 5 2 6 6 8\n",
            " 6 8 4 6 5 7 2 3 8 3 9 3 5 4 5 8 3 4 1 4 6 2 4 3 6 5 5 1 4 7 5 1 5 7 1 9 7\n",
            " 9 3 5 2 2 9 6 1 3 4 8 7 5 4 4 1 9 2 4 5 8 4 2 9 1 7 9 5 2 2 5 4 9 7 4 4 7\n",
            " 4 4 5 1 1 3 4 9 8 1 7 7 1 6 1 3 6 3 7 4 4 2 9 1 4 1 2 6 3 5 6 1 9 9 6 2 7\n",
            " 9 5 8 2 6 9 6 5 9 9 2 4 6 9 9 2 7 2 9 1 8 7 2 2 2 6 3 1 9 5 6 6 7 7 3 5 8\n",
            " 7 2 7 1 8 7 9 2 3 5 4 5 6 9 5 8 9 8 1 6 2 7 1 9 1 3 3 2 5 3 3 2 8 1 9 5 3\n",
            " 5 8 9 3 3 6 3 2 2 9 6 2 6 1 1 3 6 8 8 4 5 4 6 5 4 6 5 5 9 1 7 1 1 2 2 9 9\n",
            " 3 7 8 7 2 9 6 3 8 6 7 5 5 7 7 2 1 3 6 5 1 4 6 4 4 4 3 6 3 4 3 2 7 7 7 7 2\n",
            " 1 7 9 7 6 4 1 4 9 7 8 5 5 3 7 4 5 5 1 4 4 4 7 5 7 7 7 5 7 9 2 1 1 3 2 8 7\n",
            " 9 2 4 9 3 2 2 1 9 8 9 1 4 7 3 4 3 5 2 3 2 1 8 9 7 8 5 3 6 2 4 5 2 1 5 6 2\n",
            " 5 5 2 7 1 7 2 2 2 3 8 8 1 9 7 8 6 5 8 1 9 1 3 7 2 8 9 6 9 1 1 4 5 4 3 7 3\n",
            " 4 8 8 6 6 9 7 7 9 1 6 8 4 8 5 5 9 8 5 8 7 8 8 3 1 1 2 5 1 4 6 7 5 8 5 5 3\n",
            " 1 5 8 4 2 5 3 5 3 6 7 1 7 9 7 3 3 1 3 3 7 1 4 7 7 5 2 7 3 8 4]\n",
            "you: [3 8 5 9 2 2 8 7 9 2 1 5 9 5 3 7 7 5 3 4 6 2 9 3 8 7 6 3 7 9 8 1 1 6 8 5 1\n",
            " 8 6 3 8 8 1 2 3 8 9 9 1 1 8 7 9 8 6 6 9 9 9 3 1 7 7 2 8 1 1 9 2 8 1 6 9 2\n",
            " 7 6 5 6 5 5 7 8 1 6 3 4 1 9 7 6 1 7 4 7 3 7 2 3 2 6 2 3 9 7 2 8 6 1 3 1 3\n",
            " 3 6 8 6 1 4 3 3 5 9 8 6 7 8 5 8 4 6 4 7 5 6 7 6 1 4 4 9 6 8 8 1 8 4 2 2 9\n",
            " 3 2 2 8 4 9 5 1 3 3 2 9 7 2 3 4 4 4 6 8 1 9 4 9 7 9 1 4 7 9 3 9 5 5 4 4 4\n",
            " 6 5 3 9 2 6 8 4 8 5 9 8 7 9 5 6 9 6 7 9 3 8 1 2 6 4 2 5 2 9 1 6 3 9 8 7 1\n",
            " 4 9 2 5 7 4 6 5 1 6 6 2 2 2 7 6 1 9 1 4 3 9 1 7 1 3 9 4 2 5 9 1 4 4 3 3 8\n",
            " 8 7 4 6 5 9 8 2 5 7 6 5 8 9 7 2 4 2 7 7 8 8 2 9 6 2 6 3 4 3 5 1 7 3 2 9 6\n",
            " 3 6 4 1 8 2 8 3 5 7 7 3 6 2 1 5 7 8 7 5 7 3 5 1 4 2 9 4 6 8 4 1 6 9 9 3 9\n",
            " 9 6 5 3 8 1 5 9 7 6 4 2 7 1 7 3 3 7 4 1 7 8 3 7 9 4 6 3 1 7 2 2 1 7 9 7 7\n",
            " 6 5 2 4 8 8 4 1 8 4 8 8 6 9 9 8 1 7 1 4 9 8 6 8 2 3 6 2 2 1 5 1 1 8 7 3 3\n",
            " 8 5 5 4 6 8 1 9 7 9 1 7 9 9 1 8 5 3 2 1 8 4 2 7 8 4 6 4 4 1 6 5 4 5 1 7 1\n",
            " 2 3 9 2 5 7 9 1 2 2 5 1 9 4 4 7 9 3 2 4 9 6 5 1 4 1 3 2 2 4 8 6 6 4 9 6 6\n",
            " 2 9 5 1 9 5 6 9 9 5 1 5 8 5 7 9 6 2 3 6 4 7 4 5 4 5 4 6 7 2 7]\n",
            "(3, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Initializing the Weight Matricies\n",
        "\n",
        "Each input has 3 weight matrices:\n",
        "\n",
        "*   $W_Q$\n",
        "*   $W_K$\n",
        "*   $W_V$\n",
        "\n",
        "These 3 weight matrices are applied to all the inputs in the model to project the positional embeddings into a query, key and value matrix (i.e. QKV). Since we have eight heads in this model each dimension is 64 (i.e. 512 divided by 8 is 64). *Importantly, the dimensionality of both the query and key columns of the matrix have to be the same; the column dimensionality of the value vector does not need to be the same.*"
      ],
      "metadata": {
        "id": "e_pxTc-AKpIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Step 2: Create Query, Key Weights of 64 and Value Weights of 64 dimensions\")\n",
        "w_q = np.random.randint(1, 2, size=(512, 64)) # weight matrix query\n",
        "w_k = np.random.randint(3, 4, size=(512, 64)) # weight matrix key\n",
        "w_v = np.random.randint(5, 6, size=(512, 64)) # weight matrix value\n",
        "print(\"W_q\")\n",
        "print(w_q.shape)\n",
        "print(\"W_k\")\n",
        "print(w_k.shape)\n",
        "print(\"W_v\")\n",
        "print(w_v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__JhT9--Kh7r",
        "outputId": "470d9386-3558-4078-9db3-8651d1cd06ab"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2: Create Query, Key Weights of 64 and Value Weights of 64 dimensions\n",
            "W_q\n",
            "(512, 64)\n",
            "W_k\n",
            "(512, 64)\n",
            "W_v\n",
            "(512, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Matrix multiplication to obtain Q,K, and V\n",
        "\n",
        "As detailed in the book [Transformers for NLP by Denis Rothman](https://www.packtpub.com/en-us/product/transformers-for-natural-language-processing-second-edition-9781803247335?type=print&gad_source=1&gclid=Cj0KCQiA5-uuBhDzARIsAAa21T_Vro5RuNsURN8t_PUHd6aybZm0mi2BApbIxHRjw-qbjQxcrWbvRL8aApJMEALw_wcB):\n",
        "\n",
        "> We will now multiply the input vectors by the weight matrices to obtain a query, key, and value vector for each input.In this model,*we will assume that there is one $W_Q$, $W_K$, and $W_V$ matrix for all inputs. Other approaches are possible*\n",
        "\n"
      ],
      "metadata": {
        "id": "inGlzVVoYjCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
        "print(\"Query: x * w_query\")\n",
        "Q=np.matmul(x, w_q)\n",
        "print(Q.shape)\n",
        "print(\"Key: x * w_key\")\n",
        "K=np.matmul(x, w_k)\n",
        "print(K.shape)\n",
        "print(\"Value: x * w_value\")\n",
        "V=np.matmul(x, w_v)\n",
        "print(V.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BytHwKQfYir9",
        "outputId": "e49cc816-f8b1-483b-f193-b2181a543f68"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3: Matrix multiplication to obtain Q,K,V\n",
            "Query: x * w_query\n",
            "(3, 64)\n",
            "Key: x * w_key\n",
            "(3, 64)\n",
            "Value: x * w_value\n",
            "(3, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Scaled Attention Scores\n",
        "\n",
        "As detailed in the book [Transformers for NLP by Denis Rothman](https://www.packtpub.com/en-us/product/transformers-for-natural-language-processing-second-edition-9781803247335?type=print&gad_source=1&gclid=Cj0KCQiA5-uuBhDzARIsAAa21T_Vro5RuNsURN8t_PUHd6aybZm0mi2BApbIxHRjw-qbjQxcrWbvRL8aApJMEALw_wcB):\n",
        "\n",
        "> The attention head now implements the original Transformer equation: Attention(Q,K,V) = $softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V$\n",
        "\n",
        "\n",
        "> *Step 4* focuses on the first portion (i.e. without softmax): $(\\frac{QK^{T}}{\\sqrt{d_k}})$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OokrBq2edc9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Step 4: Scaled Attention Scores\")\n",
        "srt_dk = 8 # square root of 64\n",
        "attention_scores = (Q @ K.transpose())/srt_dk # matrix multiplication\n",
        "print(attention_scores.shape)\n",
        "print(f\"scaled attention score for word: I {attention_scores[0]}\")\n",
        "print(f\"scaled attention score for word: love {attention_scores[1]}\")\n",
        "print(f\"scaled attention score for word: you {attention_scores[2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbDqa30jtzCq",
        "outputId": "505421fa-5bce-4aff-f956-1238195e2cfe"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4: Scaled Attention Scores\n",
            "(3, 3)\n",
            "scaled attention score for word: I [1.49520384e+08 1.49939712e+08 1.56768768e+08]\n",
            "scaled attention score for word: love [1.49939712e+08 1.50360216e+08 1.57208424e+08]\n",
            "scaled attention score for word: you [1.56768768e+08 1.57208424e+08 1.64368536e+08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Scaled Softmax Attention Scores for each Vector\n",
        "\n",
        "*Step 5* applying softmax function to attention scores"
      ],
      "metadata": {
        "id": "NdRGWsNgyKUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Step 5: Scaled softmax attention_scores for each vector\")\n",
        "sft_attention = softmax(attention_scores)\n",
        "print(sft_attention.shape)\n",
        "print(f\"softmax attention score for word: I {softmax(attention_scores[0])}\")\n",
        "print(f\"softmax attention score for word: love {softmax(attention_scores[1])}\")\n",
        "print(f\"softmax attention score for word: you {softmax(attention_scores[2])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chj80wSIyJwN",
        "outputId": "699c648e-58ed-4887-802a-d995069031f2"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5: Scaled softmax attention_scores for each vector\n",
            "(3, 3)\n",
            "softmax attention score for word: I [0. 0. 1.]\n",
            "softmax attention score for word: love [0. 0. 1.]\n",
            "softmax attention score for word: you [0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: The Context Vector for One Word\n",
        "\n",
        "*Step 6* applying V to the softmax attention scores and then summing the results produces the output/context vector (i.e. $d_v$) for one word (e.g., \"I\") for one attention head."
      ],
      "metadata": {
        "id": "ac4MH3va1fCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Step 6: Context vector:\")\n",
        "print(f\"Value shape: {V.shape}\")\n",
        "print(f\"Softmax attention shape: {sft_attention.shape}\")\n",
        "print(f\"Context vector for word: I {(sft_attention[0][0]*V[0]+sft_attention[0][1]*V[1]+sft_attention[0][2]*V[2])}\")\n",
        "print(f\"Context vector shape: {(sft_attention[0][0]*V[0]+sft_attention[0][1]*V[1]+sft_attention[0][2]*V[2]).shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-78D5O71erU",
        "outputId": "2413f962-d37a-4752-d51e-d19d48865e10"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6: Context vector:\n",
            "Value shape: (3, 64)\n",
            "Softmax attention shape: (3, 3)\n",
            "Context vector for word: I [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Context vector shape: (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Repeat steps 1 to 6 for all other words/inputs"
      ],
      "metadata": {
        "id": "E29tNbyEGN9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Output all of the heads of the attention sublayer"
      ],
      "metadata": {
        "id": "cec2IhTRkZ9F"
      }
    }
  ]
}